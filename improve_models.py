#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script d'am√©lioration des mod√®les

Ce script impl√©mente une strat√©gie d'Undersampling (r√©duction des classes majoritaires)
pour g√©rer le d√©s√©quilibre du dataset et am√©liorer la d√©tection des classes minoritaires (ex: Action).
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    f1_score,
    confusion_matrix,
    precision_recall_fscore_support
)
from sklearn.utils import resample
import joblib
import time
import matplotlib.pyplot as plt
import seaborn as sns

print("="*80)
print("AM√âLIORATION DES MOD√àLES - UNDERSAMPLING (M√âTHODE DU TP)")
print("="*80)
print("\nüìö R√©f√©rence: TP 'D√©tection de Fraudes' - Mme Rakia JAZIRI")
print("   Solution enseign√©e: Sous-√©chantillonner la classe majoritaire")
print("="*80)

start_time = time.time()

# ============================================================================
# √âTAPE 1: CHARGEMENT DES DONN√âES
# ============================================================================
print("\n" + "="*80)
print("√âTAPE 1: CHARGEMENT DES DONN√âES")
print("="*80)

df = pd.read_csv('data/processed/cleaned_data.csv')
print(f"\nDataset charg√©: {len(df):,} √©chantillons")

X = df['Plot']
y = df['Genre']

print(f"\nüìä Distribution ORIGINALE:")
dist_original = y.value_counts().sort_index()
for genre, count in dist_original.items():
    pct = (count / len(y)) * 100
    print(f"   {genre:8s}: {count:5,} ({pct:5.2f}%)")

print(f"\n‚ö†Ô∏è  PROBL√àME IDENTIFI√â:")
print(f"   - Classe minoritaire 'action': {(y == 'action').sum():,} (5.87%)")
print(f"   - Classe majoritaire 'unknown': {(y == 'unknown').sum():,} (32.55%)")
print(f"   - Ratio d√©s√©quilibre: 1 action pour 5.5 unknown")

# ============================================================================
# √âTAPE 2: TRAIN/TEST SPLIT
# ============================================================================
print("\n" + "="*80)
print("√âTAPE 2: TRAIN/TEST SPLIT STRATIFI√â")
print("="*80)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTrain set: {len(X_train):,} √©chantillons")
print(f"Test set:  {len(X_test):,} √©chantillons")

# ============================================================================
# √âTAPE 3: UNDERSAMPLING (M√âTHODE DU TP)
# ============================================================================
print("\n" + "="*80)
print("√âTAPE 3: UNDERSAMPLING - M√âTHODE DU TP FRAUDES")
print("="*80)

print("\nüìö Citation du TP:")
print('   "Pour r√©√©quilibrer les donn√©es, nous allons garder al√©atoirement')
print('    5000 transactions de cartes normales (classe 0) et toutes les')
print('    transactions aberrantes (classe 1)."')

print("\nüéØ Application √† notre projet:")
print("   - Garder TOUS les films 'action' (classe minoritaire)")
print("   - R√©duire les autres classes √† 1,500 √©chantillons chacune")

# Cr√©er un DataFrame temporaire pour le train set
df_train = pd.DataFrame({'Plot': X_train, 'Genre': y_train})

# S√©parer par classe
df_action = df_train[df_train['Genre'] == 'action']
df_comedy = df_train[df_train['Genre'] == 'comedy']
df_drama = df_train[df_train['Genre'] == 'drama']
df_horror = df_train[df_train['Genre'] == 'horror']
df_unknown = df_train[df_train['Genre'] == 'unknown']

print(f"\nüìä Distribution AVANT undersampling:")
print(f"   action:  {len(df_action):,}")
print(f"   comedy:  {len(df_comedy):,}")
print(f"   drama:   {len(df_drama):,}")
print(f"   horror:  {len(df_horror):,}")
print(f"   unknown: {len(df_unknown):,}")
print(f"   TOTAL:   {len(df_train):,}")

# Sous-√©chantillonner √† 1,500 (sauf action qu'on garde tous)
TARGET_SIZE = 1500

df_comedy_sampled = resample(
    df_comedy,
    n_samples=TARGET_SIZE,
    random_state=42,
    replace=False
)
df_drama_sampled = resample(
    df_drama,
    n_samples=TARGET_SIZE,
    random_state=42,
    replace=False
)
df_horror_sampled = resample(
    df_horror,
    n_samples=min(TARGET_SIZE, len(df_horror)),  # Horror < 1500
    random_state=42,
    replace=(len(df_horror) < TARGET_SIZE)  # Oversampling si n√©cessaire
)
df_unknown_sampled = resample(
    df_unknown,
    n_samples=TARGET_SIZE,
    random_state=42,
    replace=False
)

# Recombiner
df_balanced = pd.concat([
    df_action,           # Tous gard√©s
    df_comedy_sampled,
    df_drama_sampled,
    df_horror_sampled,
    df_unknown_sampled
], axis=0)

# Shuffle
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

X_train_balanced = df_balanced['Plot']
y_train_balanced = df_balanced['Genre']

print(f"\nüìä Distribution APR√àS undersampling:")
dist_balanced = y_train_balanced.value_counts().sort_index()
for genre, count in dist_balanced.items():
    pct = (count / len(y_train_balanced)) * 100
    print(f"   {genre:8s}: {count:5,} ({pct:5.2f}%)")
print(f"   TOTAL:   {len(df_balanced):,}")

print(f"\n‚úÖ Dataset r√©√©quilibr√©!")
print(f"   R√©duction: {len(df_train):,} ‚Üí {len(df_balanced):,} √©chantillons")
print(f"   Gain: Plus √©quilibr√© pour la classe 'action'")

# ============================================================================
# √âTAPE 4: VECTORISATION TF-IDF
# ============================================================================
print("\n" + "="*80)
print("√âTAPE 4: VECTORISATION TF-IDF")
print("="*80)

vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),  # Bigrams
    min_df=2,
    max_df=0.8,
    sublinear_tf=True,
    strip_accents='unicode',
    lowercase=True
)

X_train_balanced_tfidf = vectorizer.fit_transform(X_train_balanced)
X_test_tfidf = vectorizer.transform(X_test)

print(f"\n‚úì Matrices TF-IDF cr√©√©es:")
print(f"   Train (balanced): {X_train_balanced_tfidf.shape}")
print(f"   Test (inchang√©):  {X_test_tfidf.shape}")

# Sauvegarder le vectorizer
joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')
print(f"   ‚úì Vectorizer sauvegard√©: models/tfidf_vectorizer.pkl")

# ============================================================================
# √âTAPE 5: ENTRA√éNEMENT DES MOD√àLES
# ============================================================================
print("\n" + "="*80)
print("√âTAPE 5: ENTRA√éNEMENT DES MOD√àLES (avec dataset √©quilibr√©)")
print("="*80)

models = {
    'Naive Bayes': MultinomialNB(alpha=1.0),
    'SVM': LinearSVC(
        C=1.0,
        class_weight='balanced',  # Principe du Boosting (Slide 84)
        max_iter=1000,
        random_state=42
    ),
    'Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=20,  # √âviter overfitting
        min_samples_split=10,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
}

results_improved = []

for model_name, model in models.items():
    print(f"\n{'='*80}")
    print(f"üöÄ {model_name} (IMPROVED)")
    print('='*80)

    # Entra√Ænement
    print(f"   Entra√Ænement en cours...")
    model.fit(X_train_balanced_tfidf, y_train_balanced)

    # Pr√©diction sur le test set
    y_pred = model.predict(X_test_tfidf)

    # M√©triques globales
    accuracy = accuracy_score(y_test, y_pred)
    f1_weighted = f1_score(y_test, y_pred, average='weighted')

    print(f"\n   ‚úì Accuracy: {accuracy*100:.2f}%")
    print(f"   ‚úì F1-Score (weighted): {f1_weighted*100:.2f}%")

    # M√©triques PAR CLASSE (Slide 154 - D√©tection Anomalies)
    print(f"\n   üìä Rapport PAR CLASSE (Slide 154 - M√©triques adapt√©es):")

    precision, recall, f1, support = precision_recall_fscore_support(
        y_test, y_pred, labels=sorted(y_test.unique()), zero_division=0
    )

    for i, genre in enumerate(sorted(y_test.unique())):
        emoji = {'action': 'üí•', 'comedy': 'üòÇ', 'drama': 'üé≠',
                 'horror': 'üëª', 'unknown': '‚ùì'}.get(genre, 'üé¨')
        print(f"      {emoji} {genre:8s}: P={precision[i]*100:5.1f}% | "
              f"R={recall[i]*100:5.1f}% | F1={f1[i]*100:5.1f}% (n={support[i]})")

    # Focus sur ACTION
    action_idx = sorted(y_test.unique()).index('action')
    action_f1 = f1[action_idx]

    print(f"\n   ‚≠ê F1-Score ACTION: {action_f1*100:.2f}%")

    # Sauvegarder
    model_filename = f'models/{model_name.lower().replace(" ", "_")}_undersampled.pkl'
    joblib.dump(model, model_filename)
    print(f"   ‚úì Mod√®le sauvegard√©: {model_filename}")

    results_improved.append({
        'Model': model_name,
        'Accuracy': accuracy,
        'F1-Weighted': f1_weighted,
        'F1-Action': action_f1
    })

# ============================================================================
# √âTAPE 6: COMPARAISON AVANT/APR√àS
# ============================================================================
print("\n" + "="*80)
print("√âTAPE 6: COMPARAISON AVANT (original) / APR√àS (undersampling)")
print("="*80)

# R√©sultats AVANT (mod√®les originaux)
results_before = pd.DataFrame([
    {'Model': 'Naive Bayes', 'Accuracy': 0.6357, 'F1-Weighted': 0.6229, 'F1-Action': 0.3862},
    {'Model': 'SVM', 'Accuracy': 0.6138, 'F1-Weighted': 0.6141, 'F1-Action': 0.35},
    {'Model': 'Random Forest', 'Accuracy': 0.5635, 'F1-Weighted': 0.5570, 'F1-Action': 0.32}
])

results_after = pd.DataFrame(results_improved)

print("\nüìä TABLEAU COMPARATIF:")
print("\n" + "-"*100)
print(f"{'Mod√®le':<18} {'Accuracy AVANT':<15} {'Accuracy APR√àS':<15} {'Œî Acc':<10} "
      f"{'F1-Action AVANT':<17} {'F1-Action APR√àS':<17} {'Œî F1-Action'}")
print("-"*100)

for i in range(len(results_before)):
    before = results_before.iloc[i]
    after = results_after.iloc[i]

    acc_before = before['Accuracy'] * 100
    acc_after = after['Accuracy'] * 100
    acc_delta = acc_after - acc_before

    f1_before = before['F1-Action'] * 100
    f1_after = after['F1-Action'] * 100
    f1_delta = f1_after - f1_before

    indicator = '‚úÖ' if f1_delta > 5 else ('‚ö†Ô∏è' if f1_delta > 0 else '‚ùå')

    print(f"{before['Model']:<18} {acc_before:6.2f}%          {acc_after:6.2f}%          "
          f"{acc_delta:+6.2f}%    {f1_before:6.2f}%            {f1_after:6.2f}%            "
          f"{f1_delta:+6.2f}%  {indicator}")

print("-"*100)

# G√©n√©rer matrice de confusion pour le meilleur mod√®le
best_idx = results_after['F1-Action'].argmax()
best_model_name = results_after.iloc[best_idx]['Model']

print(f"\nüèÜ Meilleur mod√®le pour ACTION: {best_model_name}")
print(f"   F1-Score ACTION: {results_after.iloc[best_idx]['F1-Action']*100:.2f}%")

# ============================================================================
# R√âSUM√â FINAL
# ============================================================================
elapsed_time = time.time() - start_time

print("\n" + "="*80)
print("‚úÖ AM√âLIORATION TERMIN√âE AVEC SUCC√àS!")
print("="*80)

print(f"\n‚è±Ô∏è  Temps d'ex√©cution: {elapsed_time:.1f} secondes")

print("\nüìÅ Fichiers g√©n√©r√©s:")
print("   ‚úì models/naive_bayes_undersampled.pkl")
print("   ‚úì models/svm_undersampled.pkl")
print("   ‚úì models/random_forest_undersampled.pkl")

print("\nüéØ R√âSULTATS:")
avg_gain = results_after['F1-Action'].mean() - results_before['F1-Action'].mean()
print(f"   - Gain moyen F1-Score ACTION: {avg_gain*100:+.2f}%")
print(f"   - M√©thode: Undersampling (TP D√©tection de Fraudes)")
print(f"   - Dataset r√©duit: {len(df_train):,} ‚Üí {len(df_balanced):,} √©chantillons")

print("\nüí° JUSTIFICATION P√âDAGOGIQUE:")
print("   Cette am√©lioration se base sur:")
print("   1. TP 'D√©tection de Fraudes' (Undersampling de la classe majoritaire)")
print("   2. Slide 84 - Boosting (class_weight='balanced')")
print("   3. Slide 154 - M√©triques adapt√©es (Precision/Recall par classe)")

print("\nüìä POUR ALLER PLUS LOIN:")
print("   - Tester SMOTE (oversampling) comme extension")
print("   - Optimiser les hyperparam√®tres (Grid Search)")
print("   - Analyser la courbe Precision-Recall pour Action")

print("\n" + "="*80)
